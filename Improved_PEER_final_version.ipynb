{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "249f9eb4-ce9a-4848-83d2-e85a127780f8",
   "metadata": {},
   "source": [
    "# PEER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "51fcaf56-6ed3-4ce6-afdd-b9e6cfdbe3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer\n",
    "import math\n",
    "from einops import einsum\n",
    "from tqdm import tqdm \n",
    "from einops.layers.torch import Rearrange\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch import distributed as dist\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3a08be2-fa05-49d7-a1a0-a0b0718cea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class PileDataset(Dataset):\n",
    "    def __init__(self, tokenizer, dataset_name='c4', split='train', max_length=512, num_samples=150_000):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # Load dataset depending on name\n",
    "        if dataset_name == 'c4':\n",
    "            self.data = load_dataset(\"c4\", \"en\", split=split, trust_remote_code=True)\n",
    "\n",
    "        elif dataset_name == 'wikitext':\n",
    "            self.data = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "        # Filter out empty text entries\n",
    "        self.data = self.data.filter(lambda x: x and len(x['text']) > 0)\n",
    "\n",
    "        # Limit the dataset size if needed\n",
    "        self.data = self.data.select(range(min(num_samples, len(self.data))))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]['text']\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        return input_ids, attention_mask\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9161a7fb-efde-4b8b-8d71-a2930b365785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(v):\n",
    "    return v is not None\n",
    "\n",
    "def default(v, d):\n",
    "    return v if exists(v) else d\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.scale = dim ** 0.5\n",
    "        self.gamma = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(x, dim=-1) * self.scale * self.gamma\n",
    "\n",
    "# ProductKeyMemory class implements the key lookup used in product-key memory\n",
    "class ProductKeyMemory(nn.Module):\n",
    "    def __init__(self, dim, num_keys):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_keys = num_keys\n",
    "        self.keys = nn.Parameter(torch.randn(num_keys, dim // 2))  # sub-keys of dimensionality d/2\n",
    "\n",
    "    def forward(self, query):\n",
    "        query = query.view(query.shape[0], 2, -1)  # split query into two sub-queries\n",
    "        dots = torch.einsum('bkd,nd->bkn', query, self.keys)  # compute dot product with sub-keys\n",
    "        return dots.view(query.shape[0], -1)\n",
    "\n",
    "def topk_schedule(step, warmup, min_k=4, max_k=16):\n",
    "    if step < warmup:\n",
    "        return min_k + (max_k - min_k) * step // warmup\n",
    "    return max_k\n",
    "\n",
    "def temperature_schedule(step, warmup, start_temp=2.0, end_temp=0.5):\n",
    "    if step >= warmup:\n",
    "        return end_temp\n",
    "    return start_temp - (start_temp - end_temp) * step / warmup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c3807ace-1144-4a1f-965a-6ea9595dbb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PEER model module, supports multi-head product-key retrieval and singleton MLP experts\n",
    "class PEER(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        *,\n",
    "        heads=8,\n",
    "        num_experts=1_000_000, #max\n",
    "        num_experts_per_head=16,\n",
    "        activation=nn.GELU,\n",
    "        dim_key=None,\n",
    "        product_key_topk=None,\n",
    "        product_key_topk_schedule=None,\n",
    "        temperature_schedule=None,\n",
    "        separate_embed_per_head=False,\n",
    "        pre_rmsnorm=False,\n",
    "        dropout=0.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.norm = RMSNorm(dim) if pre_rmsnorm else nn.Identity()  # optional input norm\n",
    "        self.heads = heads\n",
    "        self.separate_embed_per_head = separate_embed_per_head\n",
    "        self.num_experts = num_experts\n",
    "\n",
    "        num_expert_sets = heads if separate_embed_per_head else 1\n",
    "\n",
    "        # expert weights for down and up projection (singleton MLPs)\n",
    "        self.weight_down_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "        self.weight_up_embed = nn.Embedding(num_experts * num_expert_sets, dim)\n",
    "\n",
    "        self.activation = activation()\n",
    "\n",
    "        assert (num_experts ** 0.5).is_integer(), '`num_experts` needs to be a square'\n",
    "        assert (dim % 2) == 0, 'feature dimension should be divisible by 2'\n",
    "\n",
    "        dim_key = default(dim_key, dim // 2)\n",
    "        self.num_keys = int(num_experts ** 0.5)\n",
    "\n",
    "        # Linear projection to queries for PKM routing (split into two parts)\n",
    "        self.to_queries = nn.Sequential(\n",
    "            nn.Linear(dim, dim_key * heads * 2, bias=False),\n",
    "            Rearrange('b n (p h d) -> p b n h d', p=2, h=heads)\n",
    "        )\n",
    "\n",
    "        self.product_key_topk = default(product_key_topk, num_experts_per_head)\n",
    "        self.num_experts_per_head = num_experts_per_head\n",
    "\n",
    "        # Key tensor: shape (heads, sqrt(N), 2, d/2)\n",
    "        self.keys = nn.Parameter(torch.randn(heads, self.num_keys, 2, dim_key))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('step', torch.tensor(0, dtype=torch.long))  # training step counter\n",
    "\n",
    "        self.product_key_topk_schedule = product_key_topk_schedule\n",
    "        self.temperature_schedule = temperature_schedule\n",
    "        self.expert_norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        x = self.norm(x)  # optional normalization\n",
    "        queries = self.to_queries(x)  # split into 2 queries, shape: (2, B, N, H, D)\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            expanded_mask = attention_mask[:, :, None, None]\n",
    "            expanded_mask = expanded_mask.permute(2, 0, 1, 3)\n",
    "            queries = queries * expanded_mask  # apply attention mask to query\n",
    "\n",
    "        # compute similarity between queries and keys\n",
    "        sim = einsum(queries, self.keys, 'p b n h d, h k p d -> p b n h k')\n",
    "\n",
    "        # top-k per subkey dimension\n",
    "        current_topk = self.product_key_topk_schedule(self.step.item()) if self.product_key_topk_schedule else self.product_key_topk\n",
    "        (scores_x, scores_y), (indices_x, indices_y) = [s.topk(current_topk, dim=-1) for s in sim]\n",
    "\n",
    "        # form Cartesian product of topk results\n",
    "        all_scores = scores_x.unsqueeze(-1) + scores_y.unsqueeze(-2)\n",
    "        all_indices = indices_x.unsqueeze(-1) * self.num_keys + indices_y.unsqueeze(-2)\n",
    "\n",
    "        # flatten candidate scores and indices\n",
    "        all_scores = all_scores.view(*all_scores.shape[:-2], -1)\n",
    "        all_indices = all_indices.view(*all_indices.shape[:-2], -1)\n",
    "\n",
    "        # select top-k from Cartesian product set\n",
    "        scores, pk_indices = all_scores.topk(self.num_experts_per_head, dim=-1)\n",
    "        indices = all_indices.gather(-1, pk_indices)\n",
    "\n",
    "        if self.separate_embed_per_head:\n",
    "            head_expert_offsets = torch.arange(self.heads, device=x.device) * self.num_experts\n",
    "            indices = indices + head_expert_offsets.view(1, 1, -1, 1)\n",
    "\n",
    "        # retrieve expert weights\n",
    "        weights_down = self.weight_down_embed(pk_indices)\n",
    "        weights_up = self.weight_up_embed(pk_indices)\n",
    "\n",
    "        # apply down projection and activation\n",
    "        x = einsum(x, weights_down, 'b n d, b n h k d -> b n h k d')\n",
    "        x = self.activation(x)\n",
    "\n",
    "        # normalize expert outputs\n",
    "        x = self.expert_norm(x.reshape(-1, x.shape[-1])).reshape_as(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # temperature annealed softmax for routing scores\n",
    "        current_temperature = self.temperature_schedule(self.step.item()) if self.temperature_schedule else 1.0\n",
    "        score_weights = F.softmax(scores / current_temperature, dim=-1)\n",
    "        score_weights = torch.nan_to_num(score_weights, nan=0.0, posinf=1.0, neginf=0.0)\n",
    "\n",
    "        x = x * score_weights.unsqueeze(-1)  # apply score weights\n",
    "\n",
    "        # final up projection\n",
    "        x = einsum(x, weights_up, 'b n h k d, b n h k d -> b n d')\n",
    "\n",
    "        self.step += 1\n",
    "        return x\n",
    "\n",
    "\n",
    "# TransformerBlock integrates attention + two PEER feedforward modules\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, num_experts, num_experts_per_head, dropout=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # Standard multi-head self-attention\n",
    "        self.attention = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "\n",
    "        # First PEER feedforward layer\n",
    "        self.peer1 = PEER(dim, heads=num_heads, \n",
    "                          num_experts=num_experts, \n",
    "                          num_experts_per_head=num_experts_per_head,\n",
    "                          # product_key_topk_schedule=topk_schedule,\n",
    "                          # temperature_schedule=temperature_schedule\n",
    "                         )\n",
    "\n",
    "        # Second PEER layer applied after GELU (stacked expert processing)\n",
    "        self.peer2 = PEER(dim, heads=num_heads, \n",
    "                          num_experts=num_experts, \n",
    "                          num_experts_per_head=num_experts_per_head,\n",
    "                          # product_key_topk_schedule=topk_schedule,\n",
    "                          # temperature_schedule=temperature_schedule\n",
    "                         )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        # Standard attention followed by residual connection and norm\n",
    "        attn_output, _ = self.attention(x, x, x, key_padding_mask=~attention_mask.bool() if attention_mask is not None else None)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # First PEER expert layer\n",
    "        peer_output1 = self.peer1(x)\n",
    "        # Second PEER expert layer with GELU activation in between\n",
    "        peer_output2 = self.peer2(F.gelu(peer_output1))\n",
    "\n",
    "        # Residual connection and norm\n",
    "        x = x + self.dropout(peer_output2)\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Full PEER-based language model\n",
    "class PEERLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, dim, num_layers, num_heads, num_experts, num_experts_per_head):\n",
    "        super().__init__()\n",
    "\n",
    "        # Embedding layers for tokens and positions\n",
    "        self.token_embedding = nn.Embedding(vocab_size, dim)\n",
    "        self.position_embedding = nn.Embedding(512, dim)  # fixed positional encoding\n",
    "\n",
    "        # Stack of TransformerBlocks with PEER\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerBlock(dim, num_heads, num_experts, num_experts_per_head) for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        # Final LM head\n",
    "        self.lm_head = nn.Linear(dim, vocab_size, bias=False)\n",
    "\n",
    "    def forward(self, x, attention_mask=None):\n",
    "        b, s = x.shape\n",
    "        positions = torch.arange(s, device=x.device).unsqueeze(0).expand(b, s)\n",
    "\n",
    "        # Embed tokens and positions\n",
    "        x = self.token_embedding(x) + self.position_embedding(positions)\n",
    "\n",
    "        # Pass through each transformer block\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attention_mask=attention_mask)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        # Project to vocabulary logits\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08f27f0a-0709-4bc6-b108-e8b1f548c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(model, train_loader, optimizer, device, max_batches=None, show_progress=True, grad_accum_steps=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    scaler = GradScaler()\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    loop = enumerate(train_loader)\n",
    "    if show_progress:\n",
    "        loop = tqdm(loop, total=min(len(train_loader), max_batches or len(train_loader)),\n",
    "                    desc=\"Training\", dynamic_ncols=True, unit=\"batch\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for batch_idx, (batch) in loop:\n",
    "        if max_batches is not None and batch_idx >= max_batches:\n",
    "            break\n",
    "        input_ids, attention_mask = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        with autocast():  # mixed precision context\n",
    "            logits = model(input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "            loss = loss / grad_accum_steps\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "        # Step optimizer every grad_accum_steps\n",
    "        if (batch_idx + 1) % grad_accum_steps == 0 or (max_batches and (batch_idx + 1) == max_batches):\n",
    "            clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        batch_loss = loss.item() * grad_accum_steps  # undo scaling for logging\n",
    "        batch_losses.append(batch_loss)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if show_progress:\n",
    "            loop.set_postfix(loss=f\"{batch_loss:.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / max(len(batch_losses), 1)\n",
    "    return avg_loss, batch_losses\n",
    "    \n",
    "\n",
    "def validate(model, val_loader, device, max_batches=None, show_progress=True):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    batch_losses = []\n",
    "\n",
    "    loop = val_loader\n",
    "    if show_progress:\n",
    "        loop = tqdm(loop, total=min(len(val_loader), max_batches or len(val_loader)),\n",
    "                    desc=\"Validation\", dynamic_ncols=True, unit=\"batch\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (input_ids, attention_mask) in enumerate(loop):\n",
    "            if max_batches is not None and batch_idx >= max_batches:\n",
    "                break\n",
    "\n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "\n",
    "            with autocast():  # AMP context\n",
    "                outputs = model(input_ids, attention_mask=attention_mask)\n",
    "                loss = F.cross_entropy(\n",
    "                    outputs.view(-1, outputs.size(-1)),\n",
    "                    input_ids.view(-1),\n",
    "                    ignore_index=0, reduction='sum'\n",
    "                )\n",
    "\n",
    "            token_count = (input_ids != 0).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += token_count\n",
    "            batch_losses.append(loss.item() / token_count)\n",
    "\n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, perplexity, batch_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a83b7e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available\n",
      "__CUDNN VERSION: 90100\n",
      "__Number CUDA Devices: 1\n",
      "__CUDA Device Name: NVIDIA GeForce RTX 4080\n",
      "__CUDA Device Total Memory [GB]: 16.747200512\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import GPUtil\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('GPU is available')\n",
    "    print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
    "    print('__Number CUDA Devices:', torch.cuda.device_count())\n",
    "    print('__CUDA Device Name:',torch.cuda.get_device_name(0))\n",
    "    print('__CUDA Device Total Memory [GB]:',torch.cuda.get_device_properties(0).total_memory/1e9)\n",
    "else:\n",
    "    print('GPU is not available')\n",
    "\n",
    "GPUtil.getAvailable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fe714ff-2892-40b5-ac89-fd32b6ccd832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in single GPU or notebook mode\n"
     ]
    }
   ],
   "source": [
    "def smooth_curve(data, window=100):\n",
    "    \"\"\"Simple moving average.\"\"\"\n",
    "    data = np.array(data)\n",
    "    if len(data) < window:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window) / window, mode='valid')\n",
    "\n",
    "def format_k(x, _):\n",
    "    \"\"\"Tick format for '10k' etc.\"\"\"\n",
    "    return f'{int(x/1000)}k' if x >= 1000 else str(int(x))\n",
    "\n",
    "\n",
    "def plot_epoch_metrics(train_losses, val_losses, val_perplexities, save_dir, dataset_name='unknown'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    epochs = list(range(1, len(train_losses) + 1))\n",
    "\n",
    "    # Loss Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(epochs, val_losses, label='Val Loss', marker='o')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f\"Epoch Loss — {dataset_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_loss_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    # Perplexity Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(epochs, val_perplexities, label='Val Perplexity', marker='o', color='purple')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title(f\"Epoch Perplexity — {dataset_name}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.savefig(os.path.join(save_dir, f'epoch_perplexity_{dataset_name}.png'))\n",
    "    plt.close()\n",
    "\n",
    "    \n",
    "def is_distributed_env():\n",
    "    return (\n",
    "        dist.is_available()\n",
    "        and dist.is_nccl_available()\n",
    "        and \"RANK\" in os.environ\n",
    "        and \"LOCAL_RANK\" in os.environ\n",
    "        and \"WORLD_SIZE\" in os.environ\n",
    "    )\n",
    "\n",
    "if is_distributed_env():\n",
    "    dist.init_process_group(backend=\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    device = torch.device(\"cuda\", local_rank)\n",
    "else:\n",
    "    print(\"Running in single GPU or notebook mode\")\n",
    "    local_rank = 0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8adedb34-363b-430a-bab7-38eb7c1120eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping DDP — running on single GPU or CPU\n",
      "Number of parameters: 97178624\n"
     ]
    }
   ],
   "source": [
    "#   Hyperparameters + Configs\n",
    "vocab_size = 50257\n",
    "dim = 256\n",
    "num_layers = 4\n",
    "num_heads = 4\n",
    "num_experts = 128*128\n",
    "num_experts_per_head = 8\n",
    "batch_size = 4\n",
    "grad_accum_steps = 4  \n",
    "num_epochs = 20\n",
    "learning_rate = 1e-4\n",
    "\n",
    "enable_debug_mode = True\n",
    "subset_size = 50000       \n",
    "max_batches = 5000 if enable_debug_mode else None \n",
    "\n",
    "#   Model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = PEERLanguageModel(vocab_size, dim, num_layers, num_heads, num_experts, num_experts_per_head).to(device)\n",
    "\n",
    "#   Wrap model in DDP (if applicable)\n",
    "if dist.is_initialized():\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    model = DDP(model, device_ids=[local_rank], output_device=local_rank)\n",
    "else:\n",
    "    print(\"Skipping DDP — running on single GPU or CPU\")\n",
    "\n",
    "# Print info\n",
    "if local_rank == 0:\n",
    "    print(\"Number of parameters:\", sum(p.numel() for p in model.parameters()))\n",
    "    os.makedirs('plots', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df0b9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Finalized Training Loop\n",
    "def run(enable_debug_mode = True, data = 'wiki'):\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 2  # how many bad epochs to wait before stopping\n",
    "    min_delta = 0.01  # required improvement (e.g., 1% of loss)\n",
    "    bad_epochs = 0\n",
    "\n",
    "    #   Load dataset \n",
    "    if data == 'c4':\n",
    "        train_dataset = PileDataset(tokenizer, dataset_name='c4', split='train', num_samples=150_000)\n",
    "        val_dataset = PileDataset(tokenizer, dataset_name='c4',split='validation', num_samples=10_000)\n",
    "    elif data == 'wiki':\n",
    "        train_dataset = PileDataset(tokenizer, dataset_name='wikitext', split='train', num_samples=100_000)\n",
    "        val_dataset = PileDataset(tokenizer, dataset_name='wikitext',split='validation', num_samples=5_000)\n",
    "    \n",
    "    if enable_debug_mode:\n",
    "        train_dataset = Subset(train_dataset, list(range(min(subset_size, len(train_dataset)))))\n",
    "    \n",
    "    #   Create samplers and loaders\n",
    "    if is_distributed_env():\n",
    "        train_sampler = DistributedSampler(train_dataset)\n",
    "        shuffle = False\n",
    "    else:\n",
    "        train_sampler = None\n",
    "        shuffle = True\n",
    "        \n",
    "    # Create Dataloader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, shuffle=shuffle)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Compute total and warmup steps\n",
    "    total_steps = len(train_loader) * num_epochs // grad_accum_steps\n",
    "    warmup_steps = int(0.2 * total_steps)  # e.g., 20% of training as warmup\n",
    "    \n",
    "    if local_rank == 0:\n",
    "        print(f\"Total training steps: {total_steps}, Warmup steps: {warmup_steps}\")\n",
    "        \n",
    "    # Override top-k and temperature schedules dynamically using warmup\n",
    "    for layer in model.layers:\n",
    "        for peer in [layer.peer1, layer.peer2]:\n",
    "            peer.product_key_topk_schedule = lambda step, warmup=warmup_steps: topk_schedule(step, warmup)\n",
    "            peer.temperature_schedule = lambda step, warmup=warmup_steps: temperature_schedule(step, warmup)\n",
    "\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.95), eps=1e-8)\n",
    "\n",
    "    if enable_debug_mode:\n",
    "        max_sample = max_batches\n",
    "    else:\n",
    "        max_sample = None\n",
    "\n",
    "    # Logging for plotting\n",
    "    train_epoch_losses = []\n",
    "    val_epoch_losses = []\n",
    "    val_epoch_ppls = []\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Start training\n",
    "    for epoch in range(num_epochs):\n",
    "        if train_sampler is not None:\n",
    "            train_sampler.set_epoch(epoch)\n",
    "    \n",
    "        if local_rank == 0:\n",
    "            print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "        #   Call updated train() function\n",
    "        train_loss, train_batch_losses = train(\n",
    "            model, train_loader, optimizer, device,\n",
    "            max_batches = max_sample,\n",
    "            show_progress=True,\n",
    "            grad_accum_steps=4)\n",
    "        \n",
    "        #   Validation\n",
    "        if local_rank == 0:\n",
    "            print(f\"Epoch {epoch+1}\") \n",
    "            print(f\"Train Loss: {train_loss:.4f}\")\n",
    "            print(\"Validating...\")\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                val_loss, val_perplexity, val_batch_losses = validate(model, val_loader, device)\n",
    "    \n",
    "            print(f\"Val Loss: {val_loss:.4f}, Perplexity: {val_perplexity:.4f}\")\n",
    "        \n",
    "        train_epoch_losses.append(train_loss)\n",
    "        val_epoch_losses.append(val_loss)\n",
    "        val_epoch_ppls.append(val_perplexity)\n",
    "\n",
    "        # Check for loss improvement\n",
    "        if val_loss < best_val_loss - min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            bad_epochs = 0  # reset\n",
    "            torch.save(model.state_dict(), 'best_peer_language_model.pth')\n",
    "        else:\n",
    "            bad_epochs += 1\n",
    "            if local_rank == 0:\n",
    "                print(f\"→ No significant improvement. Bad epochs: {bad_epochs}/{patience}\")\n",
    "        \n",
    "            if bad_epochs >= patience:\n",
    "                if local_rank == 0:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                break\n",
    "        \n",
    "    end_time = time.time()  # End timer\n",
    "    elapsed = end_time - start_time\n",
    "    mins, secs = divmod(elapsed, 60)\n",
    "    print(f\"\\n Total Training + Validation Time: {int(mins)} min {int(secs)} sec\")\n",
    "\n",
    "    # plot loss curves\n",
    "    plot_epoch_metrics(\n",
    "        train_losses=train_epoch_losses,\n",
    "        val_losses=val_epoch_losses,\n",
    "        val_perplexities=val_epoch_ppls,\n",
    "        save_dir='epoch_plots',\n",
    "        dataset_name=data\n",
    "    )\n",
    "    \n",
    "    # Final model save\n",
    "    if local_rank == 0:\n",
    "        torch.save(model.state_dict(), 'final_peer_language_model.pth')\n",
    "    \n",
    "    # Cleanup (only if distributed)\n",
    "    if is_distributed_env():\n",
    "        dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce9de5e-12e3-42b9-b8bb-d96ff59aa428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d4713aa-dc91-41a2-ae98-79db3ab64f23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992539a8e7c4465bbb67bed392d82d25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/1654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_66444/1527263008.py:15: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|                                                                                              | 0/1000 [00:00<?, ?batch/s]/tmp/ipykernel_66444/1527263008.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # mixed precision context\n",
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [02:11<00:00,  7.61batch/s, loss=9.8354]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 10.1845\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation:   0%|                                                                                            | 0/2500 [00:00<?, ?batch/s]/tmp/ipykernel_66444/1527263008.py:78: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # AMP context\n",
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [02:08<00:00, 19.46batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 9.6746, Perplexity: 15907.8447\n",
      "\n",
      "Epoch 2/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [02:11<00:00,  7.59batch/s, loss=7.5845]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n",
      "Train Loss: 7.9237\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [02:07<00:00, 19.64batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 6.4196, Perplexity: 613.7863\n",
      "\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [02:11<00:00,  7.60batch/s, loss=2.9154]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n",
      "Train Loss: 6.1679\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [02:04<00:00, 20.05batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.7727, Perplexity: 321.3898\n",
      "\n",
      "Epoch 4/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=8.0158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n",
      "Train Loss: 5.6084\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.5577, Perplexity: 259.2278\n",
      "\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=3.6751]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n",
      "Train Loss: 5.5023\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.4851, Perplexity: 241.0753\n",
      "\n",
      "Epoch 6/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=6.2475]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n",
      "Train Loss: 5.4468\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.3977, Perplexity: 220.8877\n",
      "\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.76batch/s, loss=4.1228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n",
      "Train Loss: 5.3423\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.20batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.3275, Perplexity: 205.9266\n",
      "\n",
      "Epoch 8/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.77batch/s, loss=3.5648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n",
      "Train Loss: 5.2365\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.14batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.2650, Perplexity: 193.4527\n",
      "\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.74batch/s, loss=5.8049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n",
      "Train Loss: 5.2028\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.2407, Perplexity: 188.8028\n",
      "\n",
      "Epoch 10/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.75batch/s, loss=3.8011]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10\n",
      "Train Loss: 5.1743\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.27batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.2121, Perplexity: 183.4799\n",
      "\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.75batch/s, loss=4.7078]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n",
      "Train Loss: 5.0953\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.28batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 5.1176, Perplexity: 166.9386\n",
      "\n",
      "Epoch 12/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=4.7742]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n",
      "Train Loss: 5.1150\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.9897, Perplexity: 146.8945\n",
      "\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=7.6620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n",
      "Train Loss: 4.9248\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.8869, Perplexity: 132.5430\n",
      "\n",
      "Epoch 14/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.72batch/s, loss=4.8937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n",
      "Train Loss: 4.8450\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.7786, Perplexity: 118.9434\n",
      "\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.75batch/s, loss=4.2224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n",
      "Train Loss: 4.7578\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.6886, Perplexity: 108.7026\n",
      "\n",
      "Epoch 16/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=3.3717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n",
      "Train Loss: 4.6749\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.11batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.6043, Perplexity: 99.9104\n",
      "\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.73batch/s, loss=5.4247]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n",
      "Train Loss: 4.5966\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.5342, Perplexity: 93.1462\n",
      "\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.74batch/s, loss=2.1768]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n",
      "Train Loss: 4.4918\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.16batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.4962, Perplexity: 89.6743\n",
      "\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.71batch/s, loss=2.2873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n",
      "Train Loss: 4.5032\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.4579, Perplexity: 86.3080\n",
      "\n",
      "Epoch 20/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████████████████████████████████████████████████████████████████| 1000/1000 [01:18<00:00, 12.75batch/s, loss=5.3701]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "Train Loss: 4.4345\n",
      "Validating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|█████████████████████████████████████████████████████████████████████████████████| 2500/2500 [01:15<00:00, 33.18batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 4.4345, Perplexity: 84.3070\n",
      "\n",
      " Total Training + Validation Time: 56 min 39 sec\n"
     ]
    }
   ],
   "source": [
    "run('c4') # 20 epoch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e58c0-c65f-4646-bb3f-99a8713ef9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e58e3e-f96e-4feb-9e77-5e7623bc238a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cea83f2-7df1-4b5b-9220-965cf6e1aa9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf3a8fb-c835-4aa1-aa61-de7d1ad55370",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b53e41-c7b6-4a09-b14f-564c8b70f740",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8541df-b2cc-4997-a0ef-6fcdbd56fbb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14e99f1-c72e-44fa-8999-e005e8f633d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caccb1ae-d2a8-4c6b-a4d7-8f504d05b7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e1a4cb-62d8-4559-b747-e6d17593b755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d318cf10-7eda-41bd-a1f0-36fefc3f5082",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1f0c8-80b3-4c6d-af50-a12a6fc40844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1209ea8-10f9-4f1b-8e55-45783912eff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (py310torch)",
   "language": "python",
   "name": "py310torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
